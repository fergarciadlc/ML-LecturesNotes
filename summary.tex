\newpage
\section{Summary}
\section*{Lineal Regression}
\subsection*{Hypothesis}
\begin{equation}
h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \cdots + \theta_n x_n 
\end{equation}
\begin{align}
h_\theta(x) &= \theta^T x \\
h_\theta(X) &= X\theta 
\end{align}
\subsection*{Cost Function}
\begin{align}
J(\theta) &= \frac{1}{2m} \sum_{i=1}^m \left(h_\theta (x^{(i)}) - y^{(i)} \right)^2 \\
J(\theta) &= \frac{1}{2m} (X\theta - \vec{y})^T (X\theta - \vec{y})
\end{align}
\subsection*{Gradient Descent}
\begin{align}
\theta_j &:= \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} \\
\theta &:= \theta - \frac{\alpha}{m}X^T (X\theta - \vec{y})
\end{align}
for j=0, 1,..., n:
\subsection*{Normal Equation}
\begin{equation}
\theta=(X^TX)^{-1}X^Ty
\end{equation}
\section*{Logistic Regression}
\subsection*{Hypothesis}
\begin{equation}
0 \leq h_\theta (x) \leq 1
\end{equation}
``Sigmoid Function," also called the ``Logistic Function":
\begin{align}
& h_\theta (x) =  g ( \theta^T x ) \\& z = \theta^T x \\& g(z) = \dfrac{1}{1 + e^{-z}}
\end{align}
\begin{align}
& h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) \\& 
P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1
\end{align}
\subsection*{Cost Function}
\begin{equation}
J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]
\end{equation}
\begin{align}
h &= g(X\theta)\\
J(\theta)  &= \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right)
\end{align}
\subsection*{Gradient Descent}
\begin{align}
\theta_j &:= \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\
\theta &:= \theta - \frac{\alpha}{m} X^{T} (g(X \theta ) - \vec{y})
\end{align}