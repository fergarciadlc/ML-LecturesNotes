\chapter{Week 5}
\section{Neural Networks Learning}
Let's first define a few variables that we will need to use:
\begin{itemize}
	\item[A)] L= total number of layers in the network
	\item[B)] $s_1$ number of units(not including bias unit) in layer 1
	\item[C)] K=number of outputs units/classes
\end{itemize}
Recall that the cost function for regularized logistic regression was:
\begin{equation}
J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2
\end{equation}
For neural  networks, it is going to bee slightly more complicated:
